Twitter収集スクリプト(試作)
PYthon3.6で書きました．
試作なのでバグが多々あるかもです．．．

仕様
・指定キーワードでのtwitter検索(OR検索などの検索設定も可)
・検索結果をメタデータ混みで保存(json形式.APIの呼び出しごとに通し番号を付けて連番保存)
・API制限にぶつかった場合，解除されるまで待って自動再開．

使用方法
1.Twitterのデベロッパーに登録して，アクセストークン他を取得
2.Twitter_Crawler.pyと同じディレクトリのconfig.iniにアクセストークンや検索設定をセット
3.Twitter_Crawler.pyを実行．

config.iniについて
[tokens]
アクセストークンなどをセット

[search_params]
検索設定をセット．前半はTwitterAPIに直接ポストする値．これらの値の意味は本家APIマニュアルを参照のこと．
一部取り上げると
q:検索クエリ．複数単語のAND検索，OR検索なども可能．詳細はAPIマニュアルへ．
since_id:これより新しいツイートのみを検索対象とする．現在取得している最新のtweetのidをセットして実行すれば，それ以降のtweetのみ取得できる．
なお，リファレンスではこのidを含まずそれ以降のtweetを収集すると書かれているが，重複するっぽいので不安ならidに1足しておいたほうがよさげ．

後半はTwitter_Crawler独自の変数．
search_count:APIを呼び出す回数．基本は制限以下の値のセットを推奨．

[other_settings]
ファイルの保存場所などその他設定
なお，すでにそのパスが存在する場合，安全のため実行を中止する仕様になっている．
save_dir_path:収集結果を保存するディレクトリの位置
save_dir_name:そのディレクトリの名前．デフォルト名は[検索ワード]_tweets
